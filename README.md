# Marsden objective 1: non-destructive heDNA

## 1. Introduction

This GitHub repository serves as a comprehensive guide for the bioinformatic and statistical analysis of the ethanol comparison project, associated with the Marsden Fast-Start funding MFP-UOO002116.

## 2. Experimental design

The primary objective of this experiment is to assess the possibility of non-destructive heDNA recovery from museum-stored marine sponge specimens through DNA extraction from the ethanol in which specimens are stored. To verify non-destructive sampling as a suitable methodological choice for specimens across the phylum Porifera, we selected one specimen from the three major classes due to differing internal skeletal structures, including Hexactinellida (hexagonal silica spicules), Demospongiae (spongin), and Calcarea (calcium carbonate spicules). A detailed list with metadata for each specimen can be found below.

| Cat. # | Class | Species | Date | Latitude | Longitude | Depth |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 35574 | Calcarea | Petrobiona sp. | 09/02/2008 | -73.1245 | 174.3205 | 321 |
| 37535 | Demospongia | Homaxinella sp. | 24/02/2008 | -72.0093 | 173.2238 | 850 |
| 37124 | Hexactinellida | Rossella fibulata | 21/02/2008 | -72.5903 | 175.3423 | 479 |

For each sepcimen, DNA was extracted from 10 tissue biopsies without removal of excess ethanol and 10 tissue biopsies whereby excess ethanol was removed by absorbance of lint-free kimwipes. To test heDNA recovery from ethanol, we extracted DNA through (1) 1 ml evaporation, (2) 1 ml centrifugation, (3) 1 ml precipitation, (4) 1 ml filtration, and (5) 10 ml filtration. For each non-destructive method, 10 replicates were processed to enable statistical comparison between treatments.

After DNA extraction, the total DNA concentration was measured via Qubit, while DNA purity was assessed by investigating the 260/280 and 260/230 absorbence ratios measured via Denovix. To amplify heDNA signals associated with fish species (Fish16SF: 5’-GACCCTATGGAGCTTTAGAC-3’; Fish16S2R: 5’-CGCTGTTATCCCTADRGTAACT-3’), we employed a metabarcoding approach. This primer set was chosen over the MiFish-U primer set, as it does not co-amplify human DNA, a potential contaminant signal for museum-stored specimens that have been handled repeatedly for morphometric analysis.

**While we're unsure at this stage, genome sequencing was also conducted on the ONT platform to assess the possibility of retrieving (mito) genome sequences from non-destructive methods. However, we'll have to wait until we receive the results later on if this will be included in the manuscript.**

## 3. Bioinformatic analysis

For the bioinformatic analysis of the metabarcoding data, we will employ a standard [cutadapt v4.4](https://cutadapt.readthedocs.io/en/stable/) and [VSEARCH v2.16.0_macos_aarch64](https://github.com/torognes/vsearch) pipeline, followed by taxonomy assignment through a local blastn search against a curated reference database generated by [CRABS v0.1.8](https://github.com/gjeunen/reference_database_creator), and a final data curation step using [tombRaider v1.0](https://github.com/gjeunen/tombRaider) to remove PCR artefacts.

## 3.1 Starting files

For this project, we need six files for the bioinformatic analysis, including three sequence files and three metadata files. File names are listed below. All files (sequencing and metadata) can be downloaded from [figshare](https://figshare.com/account/home#/projects/184573).

Sequence file names:

1. 8437-P1-00-01_S1_L001_R1_001.fastq.gz
2. 8482-P1-0-1_S1_L001_R1_001.fastq.gz
3. 8482-P2-00-01_S1_L001_R1_001.fastq.gz

Metadata file names:

1. 8437-P1metadata.fasta
2. 8482-P1metadata.fasta
3. 8482-P2metadata.fasta

Before we start with our analysis, let's reformat these files to make it easier for us. For example, we can merge all sequencing files and all metadata files into one file per type, as the code can then be run once, rather than in a loop. Additionally, we need to strip the `^` and the `$` from the metadata files, as this approach won't be used in our pipeline. First, let's merge the sequence files. To do this, we need to unzip the files prior to merging them.

```{code-block} bash
gunzip *.fastq.gz
cat *.fastq > ethanol_comparison_heDNA.fastq
rm -r *001.fastq
```

Next, we can reformat the metadata files using the python script below. At the same time, it will output the merged metadata file.

```{code-block} python
#!/usr/bin/env Python3
input_file_list = ['8437-P1metadata.fasta', '8482-P1metadata.fasta', '8482-P2metadata.fasta']
merged_input_files = []
for item in input_file_list:
   with open(item, 'r') as infile:
      for line in infile:
         line = line.lstrip('^').rstrip('$\n') + '\n'
         merged_input_files.append(line)
with open('ethanol_comparison_heDNA_metadata.fasta', 'w') as outfile:
   for item in merged_input_files:
      outfile.write(item)
```

## 3.2 Raw sequence data quality check

Before we get started with the bioinformatic analysis, we will check the quality and integrity of the raw sequence file using [FastQC v0.12.1](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/).

```{code-block} bash
mkdir 1.fastqc_raw
fastqc ethanol_comparison_heDNA.fastq -o 1.fastqc_raw
```

![FastQC - Raw](figures/fastqc_raw.png)

## 3.3 Demultiplexing

Once quality is checked, we can assign sequences to their respective samples, i.e., demultiplexing. For this step, we will use cutadapt. At the same time as removing the barcodes from the amplicon, we will remove primer-binding regions as well, by setting the "adapter" to the barcode sequence + primer sequence.

```{code-block} bash
mkdir 2.demultiplex
cutadapt ethanol_comparison_heDNA.fastq -g file:'ethanol_comparison_heDNA_metadata.fasta' -o 2.demultiplex/{name}.fastq --discard-untrimmed --no-indels -e 0 --cores=0
```

```{admonition}
=== Summary ===

Total reads processed:              36,349,307
Reads with adapters:                24,149,058 (66.4%)

== Read fate breakdown ==
Reads discarded as untrimmed:       12,200,249 (33.6%)
Reads written (passing filters):    24,149,058 (66.4%)

Total basepairs processed: 10,941,141,407 bp
Total written (filtered):  4,440,167,368 bp (40.6%)
```

## 3.4 Quality filtering

We can further filter the remaining reads on several quality parameters using VSEARCH, including minimum length, maximum length, maximum expected errors, and maximum number of ambiguous bases. Prior to quality filtering, rename sequence headers to include sample ID and merge all files.

```{code-block} bash
mkdir 3.renamed 4.combined
for fq in 2.demultiplex/*.fastq
do
relabel=${fq/2.demultiplex\//}
vsearch --fastq_filter ${fq} --fastqout ${fq/2.demultiplex/3.renamed/} --relabel ${relabel/.fastq/}.
done
cat 3.renamed/*.fastq > 4.combined/ethanol_comparison_heDNA_combined.fastq
```

After renaming the sequence ID's for each file and all samples merged, we can check the quality and amplicon size range using FastQC. This will be essential to set the minimum and maximum read length thresholds during quality filtering.

```{code-block} bash
mkdir 5.fastqc_pre_filter
fastqc 4.combined/ethanol_comparison_heDNA_combined.fastq -o 5.fastqc_pre_filter
```

![FastQC - Pre-Filter](figures/fastqc_pre_filter.png)

```{code-block} bash
mkdir 6.filtered
vsearch --fastq_filter 4.combined/ethanol_comparison_heDNA_combined.fastq --fastq_maxee 1.0 --fastq_minlen 194 --fastq_maxlen 214 --fastq_maxns 0 --fastqout 6.filtered/ethanol_comparison_heDNA_combined_filtered.fastq --fastaout 6.filtered/ethanol_comparison_heDNA_combined_filtered.fasta
```

```{admonition}
vsearch v2.16.0_macos_aarch64, 16.0GB RAM, 8 cores
https://github.com/torognes/vsearch
Reading input file 100%  
20363664 sequences kept (of which 0 truncated), 3724301 sequences discarded.
```

After quality filtering, we can run FastQC again to check if quality filtering was successful.

```{code-block} bash
mkdir 7.fastqc_post_filter
fastqc 6.filtered/ethanol_comparison_heDNA_combined_filtered.fastq -o 7.fastqc_post_filter
```

![FastQC - Post-Filter](figures/fastqc_post_filter.png)

### 3.5 Dereplication

We will now use the fasta file to dereeplicate the data using VSEARCH, i.e., find unique sequences.

```{code-block} bash
mkdir 8.dereplication
vsearch --derep_fulllength 6.filtered/ethanol_comparison_heDNA_combined_filtered.fasta --relabel uniq. --output 8.dereplication/ethanol_comparison_heDNA_combined_filtered_derep.fasta --sizeout
```

```{admonition}
vsearch v2.16.0_macos_aarch64, 16.0GB RAM, 8 cores
https://github.com/torognes/vsearch
Dereplicating file 6.filtered/ethanol_comparison_heDNA_combined_filtered.fasta 100%  
4114456709 nt in 20363664 seqs, min 194, max 214, avg 202
Sorting 100%
2749957 unique sequences, avg cluster 7.4, median 1, max 1666279
Writing output file 100% 
```

### 3.6 Denoising

Using the unique sequence file, we will denoise the dataset using VSEARCH.

```{code-block} bash
mkdir 9.denoised
vsearch --cluster_unoise 8.dereplication/ethanol_comparison_heDNA_combined_filtered_derep.fasta --sizein --sizeout --relabel denoised. --centroids 9.denoised/ethanol_comparison_heDNA_combined_filtered_derep_denoised.fasta
```

```{admonition}
vsearch v2.16.0_macos_aarch64, 16.0GB RAM, 8 cores
https://github.com/torognes/vsearch
Reading file 8.dereplication/ethanol_comparison_heDNA_combined_filtered_derep.fasta 100%  
23696444 nt in 117334 seqs, min 194, max 214, avg 202
minsize 8: 2632623 sequences discarded.
Masking 100% 
Sorting by abundance 100%
Counting k-mers 100% 
Clustering 100%  
Sorting clusters 100%
Writing clusters 100% 
Clusters: 448 Size min 8, max 3894170, avg 261.9
Singletons: 0, 0.0% of seqs, 0.0% of clusters
```

### 3.7 Chimera removal

Unlike USEARCH, VSEARCH does not automatically remove chimeric sequences during denoising. Hence, we need to execute the following code to accomplish this.

```{code-block} bash
mkdir 10.final
vsearch --uchime3_denovo 9.denoised/ethanol_comparison_heDNA_combined_filtered_derep_denoised.fasta --sizein --nonchimeras 10.final/ethanol_comparison_heDNA_asvs.fasta --relabel asv.
```

```{admonition}
vsearch v2.16.0_macos_aarch64, 16.0GB RAM, 8 cores
https://github.com/torognes/vsearch
Reading file 9.denoised/ethanol_comparison_heDNA_combined_filtered_derep_denoised.fasta 100%  
90837 nt in 448 seqs, min 194, max 214, avg 203
Masking 100% 
Sorting by abundance 100%
Counting k-mers 100% 
Detecting chimeras 100%  
Found 339 (75.7%) chimeras, 109 (24.3%) non-chimeras,
and 0 (0.0%) borderline sequences in 448 unique sequences.
Taking abundance information into account, this corresponds to
44769 (0.3%) chimeras, 16737833 (99.7%) non-chimeras,
and 0 (0.0%) borderline sequences in 16782602 total sequences.
```

We now have created the ASVs for this project.

### 3.8 Count table

Now that we have created the ASVs, we can generate the count table using VSEARCH.

```{code-block} bash
vsearch --usearch_global 6.filtered/ethanol_comparison_heDNA_combined_filtered.fasta --db 10.final/ethanol_comparison_heDNA_asvs.fasta --strand plus --id 0.97 --otutabout 10.final/ethanol_comparison_heDNA_table.txt
```

```{admonition}
vsearch v2.16.0_macos_aarch64, 16.0GB RAM, 8 cores
https://github.com/torognes/vsearch
Reading file 10.final/ethanol_comparison_heDNA_asvs.fasta 100%  
22007 nt in 109 seqs, min 194, max 214, avg 202
Masking 100% 
Counting k-mers 100% 
Creating k-mer index 100% 
Searching 100%  
Matching unique query sequences: 20247244 of 20363664 (99.43%)
Writing OTU table (classic) 100%  
```

## 4. Taxonomy assignment

### 4.1 Reference database

For taxonomy assignment, we will generate a high-quality local reference database using [CRABS v0.1.9](https://github.com/gjeunen/reference_database_creator). First, we will download the NCBI taxonomy information.

```{code-block} bash
mkdir 11.reference_db
cd 11.reference_db
crabs db_download -s taxonomy
```

Next, we will download data from three online repositories, including the MiFish database, EMBL, and NCBI.

```{code-block} bash
crabs db_download -s mitofish -o mitofish.fasta
crabs db_download -s embl -db 'VRT*' --output embl_vrt.fasta
crabs db_download -s embl -db 'MAM*' --output embl_mam.fasta
crabs db_download -s ncbi -db nucleotide -q '16S[All Fields] AND (animals[filter] AND ("1"[SLEN] : "50000"[SLEN]))' -o ncbi16S.fasta -e gjeunen@gmail.com
```

Once downloaded, we merge the different output files into a single file.

```{code-block} bash
crabs db_merge -o merged_db.fasta -u yes -i ncbi16S.fasta mitofish.fasta embl_vrt.fasta embl_mam.fasta
```

We can then extract the amplicons through *in silico* PCR.

```{code-block} bash
crabs insilico_pcr -i merged_db.fasta -o insilico.fasta -f GACCCTATGGAGCTTTAGAC -r CGCTGTTATCCCTADRGTAACT
```

To retrieve amplicons without primer-binding regions, we can run the pairwise global alignment.

```{code-block} bash
crabs pga -i merged_db.fasta -o pga.fasta -db insilico.fasta -f GACCCTATGGAGCTTTAGAC -r CGCTGTTATCCCTADRGTAACT
```

Before filtering the reference database, we need to assign a taxonomic ID to each sequence.

```{code-block} bash
crabs assign_tax -i pga.fasta -o taxonomy.tsv -a nucl_gb.accession2taxid -t nodes.dmp -n names.dmp -w yes
```

First, we can dereplicate the database.

```{code-block} bash
crabs dereplicate -i taxonomy.tsv -o dereplicate.tsv -m uniq_species
```

We can now clean the database up through various parameters.

```{code-block} bash
crabs seq_cleanup -i dereplicate.tsv -o clean.tsv -e yes -s yes -na 1
```

Before exporting to NCBI format (currently not supported in CRABS v0.1.9), we will first export the reference database to SINTAX format.

```{code-block} bash
crabs tax_format -i clean.tsv -o sintax.fasta -f sintax
```

To start formatting the reference database to BLAST format, we can execute the python code below.

```{code-block} python
#! /usr/bin/env python3

import sys

inputFile = sys.argv[1]
outputFile = sys.argv[2]

newDict = {}

with open(inputFile, 'r') as infile:
  for line in infile:
    if line.startswith('>'):
      seqID = line.split(';')[0] + '\n'
      if 'CRABS_' in seqID:
        seqID = seqID.split(':')[0] + '\n'
    else:
      newDict[seqID] = line

with open(outputFile, 'w') as outfile:
  for k, v in newDict.items():
    _ = outfile.write(k)
    _ = outfile.write(v)
```

To add in the taxonomy information to the BLAST database, we can run the following code. First, we need to generate a taxonomic ID map, linking taxonomic IDs to Accession Numbers.

```{code-block} bash
tail -n +2 nucl_gb.accession2taxid | awk '{print $1, $3}' > taxIDmap.txt
```

Second, we need to download the taxonomic NCBI database.

```{code-block} bash
wget ftp://ftp.ncbi.nlm.nih.gov/blast/db/taxdb.tar.gz
tar -xvf taxdb.tar.gz
```

Finally, we can run the `makeblastdb` command, which is part of the BLAST+ Command Line Application.

```{code-block} bash
makeblastdb -in blast.fasta -dbtype nucl -parse_seqids -out blastDBCOI -taxid_map taxIDmap.txt
```

### 4.2 Assign taxonomy using blastn

Once the local reference database is generated, we can assign a taxonomic ID to each ASV by conducting a local blastn search.

```{code-block} bash
blastn -query ethanol_comparison_heDNA_asvs.fasta -db blastDBCOI -max_target_seqs 100 -perc_identity 50 -qcov_hsp_perc 50 -outfmt "6 qaccver saccver staxid sscinames length pident mismatch qcovs evalue bitscore qstart qend sstart send gapopen" -out blastn_taxonomy.txt
```

## 5. Data curation

### 5.1 *tombRaider*

After generating the three output files from the bioinformatic analysis and taxonomy assignment (1. ASV table: "ethanol_comparison_heDNA_table.txt"; 2. ASV fasta file: "ethanol_comparison_heDNA_asvs.fasta", 3. taxonomy assignment file: "blastn_taxonomy.txt"), we can further curate the data using [tombRaider v1.0](https://github.com/gjeunen/tombRaider). This step will attempt to remove all artefacts from the data before we conduct the statistical analysis.

```{code-block} bash
cd ../
tombRaider --criteria 'taxID;seqSim;coOccur' --frequency-input 10.final/ethanol_comparison_heDNA_table.txt --sequence-input 10.final/ethanol_comparison_heDNA_asvs.fasta --taxonomy-input 10.final/blastn_taxonomy.txt --frequency-output 10.final/tombRaider_ethanol_comparison_heDNA_table.txt --sequence-output 10.final/tombRaider_ethanol_comparison_heDNA_asvs.fasta --taxonomy-output 10.final/tombRaider_blastn_taxonomy.txt --log 10.final/tombRaider_log.txt --detection-threshold 5 --sort 'total read count' --similarity 50 --pairwise-alignment global --blast-format '6 qaccver saccver staxid sscinames length pident mismatch qcovs evalue bitscore qstart qend sstart send gapopen' --taxon-quality --occurrence-ratio 'count;2' --occurrence-type 'presence-absence'
```

```{admonition}
/// tombRaider | v1.0

|   Included Criteria | taxid, seqsim, cooccur
|   Excluded Criteria | pseudogene
|       Reading Files | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
|  Identify artefacts | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:18
|  Summary Statistics | 
|     Total # of ASVs | 109
|Total # of Artefacts | 54 (49.54%)
|       Artefact List | 
|    parent:    asv.2 | children:   asv.63, asv.103, asv.64, asv.66, asv.97, asv.73, asv.76, asv.90, asv.108, asv.99, asv.82, asv.104, asv.105, asv.101, asv.102
|    parent:    asv.4 | children:   asv.93, asv.94, asv.88
|    parent:    asv.6 | child:      asv.26
|    parent:    asv.7 | children:   asv.72, asv.81, asv.68, asv.109, asv.55, asv.58, asv.70
|    parent:   asv.11 | child:      asv.79
|    parent:   asv.12 | child:      asv.80
|    parent:   asv.13 | child:      asv.62
|    parent:   asv.15 | children:   asv.96, asv.91
|    parent:   asv.16 | child:      asv.24
|    parent:   asv.19 | children:   asv.83, asv.51
|    parent:   asv.20 | children:   asv.42, asv.71
|    parent:   asv.21 | child:      asv.28
|    parent:   asv.23 | children:   asv.29, asv.35, asv.47, asv.50
|    parent:   asv.27 | children:   asv.45, asv.59
|    parent:   asv.33 | child:      asv.75
|    parent:   asv.34 | children:   asv.52, asv.78
|    parent:   asv.54 | children:   asv.69, asv.98
|    parent:   asv.37 | child:      asv.43
|    parent:   asv.39 | child:      asv.40
|    parent:   asv.41 | children:   asv.53, asv.57
|    parent:   asv.49 | child:      asv.85
|    parent:   asv.67 | child:      asv.89
```

*tombRaider* will generate four files, including an updated version of the three input files and a log file.

### 5.2 ALEX

Since the BLAST output contains a maximum 100 hits for each ASV, we can use the software program [ALEX](https://github.com/gjeunen/ALEX) (Ancestor Link EXplorer) to parse the BLAST results. ALEX takes in a BLAST file, finds the most recent common ancestor, and outputs the taxonomic ID, as well as the species list of the most recent common ancestor results per ASV in a table that is ordered identical to the count table, thereby allowing easy merging.

```{code-block} bash
alex --mrca --blast-input 10.final/tombRaider_blastn_taxonomy.txt --table-input 10.final/tombRaider_ethanol_comparison_heDNA_table.txt --names-input names.dmp --nodes-input nodes.dmp --output 10.final/alex_blastn_taxonomy.txt
```

### 5.3 Negative controls

After removal of PCR artefacts, we can import all data into R. First, we explore eDNA signals present in the negative control samples.

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(Biostrings)
library(dplyr)

## read data into R
count_table <- read.table('tombRaider_ethanol_comparison_heDNA_table.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
sequence_table <- readDNAStringSet('tombRaider_ethanol_comparison_heDNA_asvs.fasta')
taxonomy_table <- read.table('alex_tombRaider_blastn_taxonomy.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
metadata_table <- read.table('ethanol_comparison_heDNA_metadata.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')

## list negative controls present and absent in count_table
negative_control_samples <- rownames(metadata_table[metadata_table$type == 'negative', ])
negatives_present_in_count <- negative_control_samples[negative_control_samples %in% colnames(count_table)]
negatives_absent_in_count <- negative_control_samples[!negative_control_samples %in% colnames(count_table)]

## explore negative control samples
for (i in seq_along(negatives_present_in_count)) {
  col_name <- negatives_present_in_count[i]
  print(col_name)
  print(sum(count_table[[col_name]]))
  print(sum(count_table[[col_name]]) / sum(count_table) * 100)
  row_names_above_zero <- rownames(count_table[count_table[[col_name]] > 0, ])
  sample_sums <- numeric(length(row_names_above_zero))
  sample_count <- numeric(length(row_names_above_zero))
  proportion_count <- numeric(length(row_names_above_zero))
  negative_reads <- numeric(length(row_names_above_zero))
  negative_reads_percentage <- numeric(length(row_names_above_zero))
  negative_taxa_id <- numeric(length(row_names_above_zero))
  for (i in seq_along(row_names_above_zero)){
    row_name <- row_names_above_zero[i]
    row_values <- count_table[row_name, setdiff(colnames(count_table), col_name)]
    sample_sums[i] <- sum(row_values)
    sample_count[i] <- sum(row_values > 0)
    negative_reads[i] <- count_table[row_name, col_name]
    negative_taxa_id[i] <- taxonomy_table[row_name, 'matching species IDs']
    negative_reads_percentage[i] <- negative_reads[i] / sample_sums[i] * 100
    proportion_count[i] <- sample_count[i] / (ncol(count_table) - length(negatives_present_in_count)) * 100
  }
  result_df <- data.frame(
    row_name = row_names_above_zero,
    taxon_id = negative_taxa_id,
    neg_reads = negative_reads,
    sample_reads = sample_sums,
    neg_proportion = negative_reads_percentage,
    positive_samples = sample_count,
    proportion_positive = proportion_count
  )
  print(result_df)
}

## remove contamination and singleton detections
count_table_clean <- subset(count_table, rownames(count_table) != 'asv.14')
count_table_clean <- as.data.frame(t(apply(count_table_clean, 1, function(row){
  rowsum_value <- sum(row)
  threshold <- 0.004 / 100 * rowsum_value
  ifelse(row < threshold, 0, row)
})))
count_table_clean[count_table_clean < 2] <- 0

## check if any samples or ASVs sum to zero and remove them
remove_0_cols <- colnames(count_table_clean)[colSums(count_table_clean) == 0]
remove_0_rows <- rownames(count_table_clean)[rowSums(count_table_clean) == 0]
count_table_clean <- count_table_clean[, !(names(count_table_clean) %in% remove_0_cols)]
count_table_clean <- count_table_clean[!(rownames(count_table_clean) %in% remove_0_rows), ]

## update other dataframes
metadata_table_clean <- metadata_table[colnames(count_table_clean), ]
sequence_table_clean <- sequence_table[rownames(count_table_clean)]
taxonomy_table_clean <- taxonomy_table[rownames(count_table_clean), ]

## export dataframes to output files
writeXStringSet(sequence_table_clean, file = 'contaminant_removed_tombRaider_ethanol_comparison_heDNA_asvs.fasta')
write.table(count_table_clean, file = 'contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', append = FALSE, sep = '\t', dec = '.', row.names = TRUE, col.names = NA)
write.table(metadata_table_clean, file = 'contaminant_removed_ethanol_comparison_heDNA_metadata.txt', append = FALSE, sep = '\t', dec = '.', row.names = TRUE, col.names = NA)
write.table(taxonomy_table_clean, file = 'contaminant_removed_alex_tombRaider_blastn_taxonomy.txt', append = FALSE, sep = '\t', dec = '.', row.names = TRUE, col.names = NA)
```

The code above shows that only 1 out of 15 control samples (sample ID: qPCRNEG) contained any reads. 52 reads were assigned to qPCRNEG, which amounts to 0.00028% of the data. Reads were assigned to five ASVs, with detailed information provided in the table below.

```{admonition}
  row_name                taxon_id neg_reads sample_reads neg_proportion positive_samples proportion_positive
1    asv.1     Chaenodraco wilsoni        13      4470614   0.0002907878               76           36.190476
2    asv.6     Cryodraco atkinsoni         4       985377   0.0004059360               91           43.333333
3    asv.7 Pleuragramma antarctica        14       786855   0.0017792351              124           59.047619
4   asv.14  Laemonema sp. ARV-2009        22       154938   0.0141992281                3            1.428571
5   asv.21      Chionodraco myersi         4       102528   0.0039013733               51           24.285714
```

Based on these results, we will remove asv.14 from the data set, since it is only present in a single sample with more than 1 read (2 out of 3 samples only positive as singletons). We will also filter the data based on an abundance threshold of 0.0039% or 1, depending on which is higher. After abundance threshold filtration, the qPCRNEG sample does not contain any reads, as well as 4 samples, including T74_1, T74_8, TW74_5, and TW74_6. All samples are tissue extractions from the calcereous sponge. Besides asv.14, which is deemed a contaminant, no additional ASVs sum to 0. Therefore, no additional ASVs are removed.

### 5.4 Unassigned and off-target ASVs

Besides contamination removal, we will also investigate ASVs without a taxonomic ID and ASVs assigned to off target species.

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(Biostrings)
library(dplyr)

## read data into R
count_table <- read.table('contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
sequence_table <- readDNAStringSet('contaminant_removed_tombRaider_ethanol_comparison_heDNA_asvs.fasta')
taxonomy_table <- read.table('contaminant_removed_alex_tombRaider_blastn_taxonomy.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
metadata_table <- read.table('contaminant_removed_ethanol_comparison_heDNA_metadata.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')

## list ASVs without a taxonomic ID and print sequences for manual BLASTing
no_tax_id <- rownames(taxonomy_table[is.na(taxonomy_table$superkingdom), ])
for (item in no_tax_id) {
  print(item)
  print(as.character(sequence_table[[item]]))
}

## list ASVs with an off-target taxonomic ID and print sequences for manual BLASTing
off_target_id <- rownames(taxonomy_table[taxonomy_table$phylum != 'Chordata' & !is.na(taxonomy_table$phylum), ])
for (item in off_target_id) {
  print(item)
  print(as.character(sequence_table[[item]]))
}

## remove ASVs in no_tax_id and off_target_id from count_table
count_table_clean <- count_table[!(rownames(count_table) %in% no_tax_id | rownames(count_table) %in% off_target_id), ]

## remove any columns that sum to 0
remove_0_cols <- colnames(count_table_clean)[colSums(count_table_clean) <1000]
count_table_clean <- count_table_clean[, !(names(count_table_clean) %in% remove_0_cols)]

## update other data frames
metadata_table_clean <- metadata_table[colnames(count_table_clean), ]
sequence_table_clean <- sequence_table[rownames(count_table_clean)]
taxonomy_table_clean <- taxonomy_table[rownames(count_table_clean), ]

## export data to files
writeXStringSet(sequence_table_clean, file = 'clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_asvs.fasta')
write.table(count_table_clean, file = 'clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', append = FALSE, sep = '\t', dec = '.', row.names = TRUE, col.names = NA)
write.table(metadata_table_clean, file = 'clean_contaminant_removed_ethanol_comparison_heDNA_metadata.txt', append = FALSE, sep = '\t', dec = '.', row.names = TRUE, col.names = NA)
write.table(taxonomy_table_clean, file = 'clean_contaminant_removed_alex_tombRaider_blastn_taxonomy.txt', append = FALSE, sep = '\t', dec = '.', row.names = TRUE, col.names = NA)
```

There are 8 ASVs that did not receive a taxonomic ID with the local BLAST search, including asv.32, asv.34, asv. 41, asv.44, asv.48, asv.84, asv.107, and asv.106. These 8 ASVs will be manually searched against the full NCBI database with an [online BLAST search](https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome) with default settings. All BLAST results came back as "No significant similarity found." and all 8 ASVs will, therefore, be removed from the analysis.

There are also 3 ASVs that received non-chordate taxonomic ID's, including asv.16, asv.17, and asv.25. These 3 ASVs will also be manually searched against the full NCBI database with an [online BLAST search](https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome) to ensure ASVs do not belong to the phylum Chordata. Results for the online BLAST search agree with results obtained from the local BLAST search, with all 3 ASVs assigned to echinoderm taxa. Given that the primers are not optimised for echinoderms and amplification will potentially highly variable, we will remove these ASVs from the analysis.

Removal of these 11 ASVs resulted in a drop of an additional 21 samples. All samples are extractions from the calcereous sponge using different extraction techniques, including E74_3, E74_7, F174_1, F174_10, F174_6, F174_7, F174_8, P74_2, P74_3, T74_10, T74_3, T74_4, T74_5, T74_9, TW74_1, TW74_10, TW74_3, TW74_4, TW74_7, TW74_8, TW74_9.

Once we have exported the updated files after data curation, we have generated the final files for statistical analysis and can move forward with the statistical analysis.

## 6. Data exploration

### 6.1 Track reads across various stages of data processing and curation

Tracking the number of total reads and reads per sample at various points during the bioinformatic analysis and data curation steps will help visualise potential issues with the code or with samples. The python code below parses the number of reads and returns a text file. The following R code plots the data for use in supplemental files.

```{code-block} python
#! /usr/bin/env Python3
import os

# raw total read count
rawcount = 0
with open('ethanol_comparison_heDNA.fastq', 'r') as rawfile:
    for line in rawfile:
        rawcount += 1
rawcount = int(rawcount / 4)

# demultiplexed total read count
demultiplexcount = 0
demuxfilelist = os.listdir('2.demultiplex')
for demuxfile in demuxfilelist:
    with open(f'2.demultiplex/{demuxfile}', 'r') as demuxf:
        for line in demuxf:
            demultiplexcount += 1
demultiplexcount = int(demultiplexcount / 4)

# filtered total read count
filtercount = 0
with open(f'6.filtered/ethanol_comparison_heDNA_combined_filtered.fastq', 'r') as filterf:
    for line in filterf:
        filtercount += 1
filtercount = int(filtercount / 4)

# pre data curation read count
curationcount = 0
with open('10.final/ethanol_comparison_heDNA_table.txt', 'r') as curationf:
    for line in curationf:
        if line.startswith('asv'):
            asvcount = line.rstrip('\n').split('\t')[1:]
            for item in asvcount:
                curationcount += int(item)

# final total read count
finalcount = 0
with open('10.final/clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', 'r') as finalf:
    for line in finalf:
        if line.startswith('asv'):
            asvcount = line.rstrip('\n').split('\t')[1:]
            for item in asvcount:
                finalcount += int(item)

# write data to output
with open('10.final/tracking_read_count_total.txt', 'r') as outfile:
    outfile.write(f'type\tcount\tproportion\nrawcount\t{rawcount}\t{(rawcount / rawcount * 100):.2f}\n')
    outfile.write(f'demuxcount\t{demultiplexcount}\t{(demultiplexcount / rawcount * 100):.2f}\n')
    outfile.write(f'filtercount\t{filtercount}\t{(filtercount / rawcount * 100):.2f}\n')
    outfile.write(f'curationcount\t{curationcount}\t{(curationcount / rawcount * 100):.2f}\n')
    outfile.write(f'finalcount\t{finalcount}\t{(finalcount / rawcount * 100):.2f}\n')
```

```{code-block} python
#! /usr/bin/env Python3
import os
import collections

# generate dictionary of all samples from metadata file
sampledict = collections.defaultdict(dict)
with open('ethanol_comparison_heDNA_metadata.fasta', 'r') as metadata:
    for line in metadata:
        if line.startswith('>'):
            sampleID = line.rstrip('\n').lstrip('>')
            sampledict[sampleID]

# read count after demultiplexing
demuxfilelist = os.listdir('2.demultiplex')
for demuxfile in demuxfilelist:
    demuxcount = 0
    with open(f'2.demultiplex/{demuxfile}', 'r') as demuxf:
        for line in demuxf:
            demuxcount += 1
    demuxcount = int(demuxcount / 4)
    demuxfileID = demuxfile.split('.')[0]
    sampledict[demuxfileID]['demuxcount'] = demuxcount

# read count after filtering
with open('6.filtered/ethanol_comparison_heDNA_combined_filtered.fasta', 'r') as filterf:
    for line in filterf:
        if line.startswith('>'):
            filterID = line.rstrip('\n').lstrip('>').split('.')[0]
            try:
                sampledict[filterID]['filtercount'] += 1
            except KeyError:
                sampledict[filterID]['filtercount'] = 1

# read count from ASV table pre data curation
mapDict = {}
with open('10.final/ethanol_comparison_heDNA_table.txt', 'r') as curationf:
    for line in curationf:
        if line.startswith('#'):
            maplist = line.rstrip('\n').split('\t')[1:]
            for i in range(len(maplist)):
                mapDict[i] = maplist[i]
        else:
            readlist = line.rstrip('\n').split('\t')[1:]
            for i in range(len(readlist)):
                try:
                    sampledict[mapDict[i]]['finalcount'] += int(readlist[i])
                except KeyError:
                    sampledict[mapDict[i]]['finalcount'] = int(readlist[i])

# read count from final ASV table
mapDict = {}
with open('10.final/clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', 'r') as finalf:
    for line in finalf:
        if line.startswith('\t'):
            maplist = line.rstrip('\n').split('\t')[1:]
            for i in range(len(maplist)):
                mapDict[i] = maplist[i]
        else:
            readlist = line.rstrip('\n').split('\t')[1:]
            for i in range(len(readlist)):
                try:
                    sampledict[mapDict[i]]['finalcount'] += int(readlist[i])
                except KeyError:
                    sampledict[mapDict[i]]['finalcount'] = int(readlist[i])

# print read counts per sample
for key, value in sampledict.items():
    valuestring = ''
    for k, v in value.items():
        valuestring += ', ' + str(k) + ': ' + str(v)
    print(f'sample ID: {key}{valuestring}')
```

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(ggplot2)
library(tidyr)
library(dplyr)
library(scales)

# Read the data from the text file
data <- read.table("tracking_read_count_total.txt", header = TRUE, sep = "\t")

# Calculate the differences in reverse order
data$difference <- c(
  data$count[1] - data$count[2],
  data$count[2] - data$count[3],
  data$count[3] - data$count[4],
  data$count[4] - data$count[5],
  data$count[5]                          
)

# Reorder data to have the stages in reverse order (from rawcount to finalcount)
data$type <- factor(data$type, levels = c("rawcount", "demuxcount", "filtercount", "curationcount", "finalcount"))

# Set specific colors for each factor
color_palette <- c(
  "rawcount" = "#BBC6C8",    
  "demuxcount" = "#469597", 
  "filtercount" = "#DDBEAA",  
  "curationcount" = "#806491", 
  "finalcount" = "#2F70AF"   
)

# Create the horizontal stacked bar chart
ggplot(data, aes(x = "Read Counts", y = difference, fill = type)) +
  geom_bar(stat = "identity", width = 0.7) +
  geom_text(aes(label = format(count, big.mark = ",", scientific = FALSE)), 
            position = position_stack(vjust = 0.5), color = "white", size = 4) +
  coord_flip() +
  scale_fill_manual(values = color_palette) +
  scale_y_log10(expand = expansion(mult = c(0, 0))) +
  theme_classic() +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), legend.position = "top", legend.text = element_text(size = 12), axis.text.x = element_text(size = 10), axis.title.x = element_blank(), axis.title.y = element_blank())
```

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(ggplot2)
library(tidyr)
library(dplyr)
library(scales)
library(stringr)

# Read the data from the text file
data <- read.table("tracking_read_count_sample.txt", header = TRUE, sep = "\t")

## calculate the differences for each stage
data <- data %>%
  mutate(
    filterdiff = demuxcount - filtercount,
    curationdiff = filtercount - curationcount,
    finaldiff = curationcount - finalcount,
    group = sub("_.*", "", sample_id),
    id = sub(".*_", "", sample_id)) %>%
  mutate(group = ifelse(group %in% c("C24", "C35", "C74", "E24", "E35", 
                                     "E74", "F024", "F035", "F074", 
                                     "F124", "F135", "F174", "P24", 
                                     "P35", "P74", "T24", "T35", 
                                     "T74", "TW24", "TW35", "TW74"), 
                        group, "Negative"))

## subset dataframe to split controls and samples
negative_df <- data %>% filter(group == "Negative")
sample_df <- data %>% filter(group != "Negative")

# Define the desired order of the sample groups
group_order <- c("T24", "TW24", "F124", "F024", "P24", "C24", "E24",
                 "T35", "TW35", "F135", "F035", "P35", "C35", "E35",
                 "T74", "TW74", "F174", "F074", "P74", "C74", "E74")
sample_order <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")

# Reshape data to long format for ggplot
sample_long <- sample_df %>%
  select(id, group, finalcount, finaldiff, curationdiff, filterdiff) %>%
  pivot_longer(cols = c(finalcount, finaldiff, curationdiff, filterdiff),
               names_to = "type", values_to = "count") %>%
  mutate(type = factor(type, levels = c("filterdiff", "curationdiff", "finaldiff", "finalcount")),
         group = factor(group, levels = group_order),
         id = factor(id, levels = sample_order))

negative_long <- negative_df %>%
  select(sample_id, group, finalcount, finaldiff, curationdiff, filterdiff) %>%
  pivot_longer(cols = c(finalcount, finaldiff, curationdiff, filterdiff),
               names_to = "type", values_to = "count")

# Create the stacked bar plot with facets
ggplot(sample_long, aes(x = id, y = count, fill = type)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_manual(values = c("finalcount" = "#2F70AF",
                               "finaldiff" = "#806491",
                               "curationdiff" = "#DDBEAA",
                               "filterdiff" = "#469597")) +
  labs(x = "Replicate number", y = "Read count") +
  theme_classic() +
  scale_y_log10()+
  theme(axis.text.y = element_text(size = 10),
        legend.title = element_blank(),
        legend.text = element_text(size = 10)) +
  facet_wrap(~ group, ncol = 7, nrow = 3)

ggplot(negative_long, aes(x = sample_id, y = count, fill = type)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_manual(values = c("finalcount" = "#2F70AF",
                               "finaldiff" = "#806491",
                               "curationdiff" = "#DDBEAA",
                               "filterdiff" = "#469597")) +
  labs(x = "Replicate number", y = "Read count") +
  theme_classic() +
  scale_y_log10()+
  theme(axis.text.y = element_text(size = 10),
        legend.title = element_blank(),
        legend.text = element_text(size = 10))
```

Once we have counted the read numbers for intermediary files, we can remove the intermediary files to save up space on the internal hard drive.

```{code-block} bash
rm -r 1.fastqc_raw 2.demultiplex 3.renamed 4.combined 5.fastqc_pre_filter 6.filtered 7.fastqc_post_filter 8.dereplication 9.denoised ethanol_comparison_heDNA.fastq
```

### 6.2 Rarefaction analysis

Besides tracking reads across various stages of data processing and curation, we can investigate if data needs to be rarefied prior to statistical analysis, i.e., equal total read count across samples. We will base the decision to rarefy data on four tests, including (i) even total read distribution across samples, (ii) a lack of a positive correlation between total read count and number of detected ZOTUs, (iii) plateauing of rarefaction curves, and (iv) curvature indices were not observed below the pre-defined threshold of 0.1.

#### 6.2.1 Read distribution across samples

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(ggplot2)
library(dplyr)
library(car)

## read data into R
count_table <- read.table('clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
raw_metadata_table <- read.table('ethanol_comparison_heDNA_metadata.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')

## add column sums of count_table to metadata_table
raw_metadata_table$total_read_count_filtered <- colSums(count_table)[rownames(raw_metadata_table)]

## plot total read count in decreasing order
omit_negative_raw_metadata_table <- raw_metadata_table %>%
  filter(sponge_id != "negative")
sample_colors <- c("calcarea" = "lightgoldenrod", "demosponge" = "steelblue", "hexactinellida" = "firebrick")
ggplot(omit_negative_raw_metadata_table, aes(x = reorder(row.names(omit_negative_raw_metadata_table), -total_read_count_filtered), y = total_read_count_filtered, fill = sponge_id)) +
  geom_bar(stat = "identity") +
  labs(x = "Sponge ID", y = "Total Read Count") +
  theme_classic() +
  scale_fill_manual(values = sample_colors) +
  scale_y_log10(breaks = 10^(seq(0, 6, by = 2)), labels = scales::comma) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "bottom") +
  facet_wrap(~ method, scales = "free_x", ncol = 4, nrow = 2)

## print basic stats of read distribution across samples
("Maximum value:", max(omit_negative_raw_metadata_tacatble$total_read_count_filtered, na.rm = TRUE), "in sample:", rownames(omit_negative_raw_metadata_table)[which.max(omit_negative_raw_metadata_table$total_read_count_filtered)], "\n")
cat("Minimum value:", min(omit_negative_raw_metadata_table$total_read_count_filtered, na.rm = TRUE), "in sample:", rownames(omit_negative_raw_metadata_table)[which.min(omit_negative_raw_metadata_table$total_read_count_filtered)], "\n")
cat("Mean value:", mean(omit_negative_raw_metadata_table$total_read_count_filtered, na.rm = TRUE), "\n")
cat("Standard error:", sd(omit_negative_raw_metadata_table$total_read_count_filtered, na.rm = TRUE) / sqrt(sum(!is.na(omit_negative_raw_metadata_table$total_read_count_filtered))), "\n")

## print basic stats of read distribution between groups
omit_negative_raw_metadata_table %>%
  group_by(sponge_id) %>%
  summarise(
    mean_count = mean(total_read_count_filtered, na.rm = TRUE),
    se_count = sd(total_read_count_filtered, na.rm = TRUE) / sqrt(n())
  )

## run one-way ANOVA, test assumptions, and plot data for visualisation
omit_negative_raw_metadata_table$total_read_count_filtered[is.na(omit_negative_raw_metadata_table$total_read_count_filtered)] <- 0
model = lm(total_read_count_filtered ~ sponge_id, data = omit_negative_raw_metadata_table)
Anova(model, type = 'II')
anova(model)
summary(model)
hist(residuals(model), col = 'grey40')
plot(fitted(model), residuals(model))
ggplot(omit_negative_raw_metadata_table, aes(x = sponge_id, y = total_read_count_filtered)) +
  geom_boxplot() +
  labs(x = "Sponge ID", y = "Total Read Count") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### 6.2.2 Correlation between read count and detected ZOTUs

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(ggplot2)
library(dplyr)
library(ggExtra)

## read data into R
raw_count_table <- read.table('ethanol_comparison_heDNA_table.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
count_table <- read.table('clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
raw_taxonomy_table <- read.table('alex_blastn_taxonomy.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
raw_metadata_table <- read.table('ethanol_comparison_heDNA_metadata.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')

## add column sums of count_table to metadata_table
raw_metadata_table$total_read_count_filtered <- colSums(count_table)[rownames(raw_metadata_table)]
raw_metadata_table$total_observations_filtered <- colSums(count_table > 0)[rownames(raw_metadata_table)]

# subset the data to remove rows with NA values
subset_data <- raw_metadata_table %>%
  filter(!is.na(total_read_count_filtered) & !is.na(total_observations_filtered))
subset_data$total_observations_filtered <- as.integer(subset_data$total_observations_filtered)
subset_data$total_read_count_filtered <- as.integer(subset_data$total_read_count_filtered)

# fit the GLM model
glm_model <- glm(total_observations_filtered ~ total_read_count_filtered, 
                 family = poisson(link = "log"), data = subset_data)
summary(glm_model)

# make predictions
subset_data$predicted <- predict(glm_model, type = "response")

# plot results
subset_data$sponge_id <- as.factor(subset_data$sponge_id)
subset_data$method <- as.factor(subset_data$method)
colour_palette <- c("centrifugation" = "#ce7c7d", "filter_1ml" = "#c4d8e1", "evaporation" = "#f2d379", "filter_10ml" = "#4e6c82", "precipitation" = "#BFB8DA", "tissue" = "grey60", "tissue_wicked" = "grey10")
p <- ggplot(subset_data, aes(x = total_read_count_filtered, y = total_observations_filtered, color = method)) +
  geom_point(aes(shape = sponge_id), size = 3) +
  scale_shape_manual(values = c("demosponge" = 15, "hexactinellida" = 18, "calcarea" = 16)) +
  scale_color_manual(values = colour_palette) +
  geom_smooth(aes(y = predicted), method = "glm", method.args = list(family = "poisson"), 
              se = TRUE, linetype = 'dotdash', color = "black", linewidth = 0.3) +
  labs(color = "extraction treatment", shape = "sponge specimen", x = "Total read count", 
       y = "Number of observed ZOTUs") +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_y_continuous(limits = c(0, 25),
                     breaks = seq(0, 25, by = 5)) +
  scale_x_log10(limits = c(1e+04, 1e+06), 
                breaks = c(1e+04, 1e+05, 1e+06), 
                labels = scales::comma)
p
p1 <- ggMarginal(p, type = 'densigram')
p1
```

#### 6.2.3 Rarefaction curves

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(ampvis2)
library(dplyr)

## read data into R
count_table <- read.table('clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', header = TRUE, sep = '\t', check.names = FALSE, comment.char = '')
colnames(count_table)[1] <- "ASV"
taxonomy_table <- read.table('alex_blastn_taxonomy.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
metadata_table <- read.table('ethanol_comparison_heDNA_metadata.txt', header = TRUE, sep = '\t', check.names = FALSE, comment.char = '')

## add taxon id to count_table with highest resolution != NA
count_table$Species <- NA
taxonomy_columns <- c("species", "genus", "family", "order", "class", "phylum", "superkingdom")
for (column in taxonomy_columns) {
  count_table$Species <- ifelse(
    is.na(count_table$Species) & !is.na(taxonomy_table[count_table$ASV, column]),
    taxonomy_table[count_table$ASV, column],
    count_table$Species
  )
}

## remove samples from metadata_table that are not in count_table
metadata_table <- metadata_table[metadata_table$sample_id %in% colnames(count_table), ]

## set method as factor for plotting
metadata_table$method <- factor(metadata_table$method, levels = c("tissue", "tissue_wicked",
                                                                  "filter_1ml", "filter_10ml",
                                                                  "centrifugation", "evaporation",
                                                                  "precipitation"))

## load dataframes into ampvis2 format
ampvis_df <- amp_load(count_table, metadata_table)

## generate rarefaction curves
sample_colors <- c("calcarea" = "lightgoldenrod", "demosponge" = "steelblue", "hexactinellida" = "firebrick")
rarefaction_curves <- amp_rarecurve(ampvis_df, stepsize = 100, color_by = 'sponge_id') +
  ylab('Number of observed ZOTUs') +
  theme_classic() +
  scale_color_manual(values = sample_colors) +
  facet_wrap(~ method, scales = "free_x", ncol = 2, nrow = 4) +
  theme(legend.position = "bottom") +
  scale_y_continuous(limits = c(0, 25),
                     breaks = seq(0, 25, by = 5)) +
  scale_x_continuous(labels = scales::comma)
rarefaction_curves
```

#### 6.2.4 Curvature indices

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(DivE)
library(sf)
library(dplyr)

## read data in R
count_table <- read.table('clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', header = TRUE, sep = '\t', check.names = FALSE, comment.char = '')

## create empty dataframe to place curvature indices in
curvature_df <- data.frame("sampleID" = numeric(0), "curvatureIndex" = numeric(0))

## iterate over columns and create new data frames
for (i in 2:ncol(count_table)) {
  col1_name <- names(count_table)[1]
  col2_name <- names(count_table)[i]
  new_df <- data.frame(count_table[, 1], count_table[, i])
  names(new_df) <- c(col1_name, col2_name)
  ## function to generate the rarefaction data from a given sample
  dss <- DivSubsamples(new_df, nrf=100, minrarefac=1, NResamples=10)
  ## calculate curvature index
  curvature_value <- Curvature(dss)
  # add info to empty dataframe
  new_row <- data.frame("sampleID" = col2_name, "curvatureIndex" = curvature_value)
  cat("item:", i-1, "/", ncol(count_table), "; sample:", col2_name, "; curvature index: ", curvature_value, "\n")
  curvature_df <- rbind(curvature_df, new_row)
}

## output table with curvature indices
write.table(curvature_df, 'curvatureIndices.txt', append = FALSE, sep = '\t', dec = '.', row.names = FALSE)
```

### 6.3 Species accumulation curves

To test if sufficient replicates were collected per sample and per treatment, we can draw species accumulation curves. Furthermore, we can also estimate the percentage of species recovered with our level of replication using inter- and extrapolation analyses.

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(iNEXT.3D)
library(ggplot2)

## read data in R
count_table <- read.table('clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', header = TRUE, row.names = 1, sep = '\t', check.names = FALSE, comment.char = '')
metadata_table <- read.table('ethanol_comparison_heDNA_metadata.txt', header = TRUE, row.names = 1, sep = '\t', check.names = FALSE, comment.char = '')

## subset metadata_table to remove negatives, fill out count_table with missing samples, transform count_table to presence-absence
metadata_table <- metadata_table[metadata_table$type != "negative", ]
for (col in setdiff(rownames(metadata_table), colnames(count_table))) {
  count_table[[col]] <- 0
}
count_table[count_table > 0] <- 1

## generate format that can be read into iNEXT.3D (datatype = "incidence_raw" --> list of matrices)
metadata_table$distinct <- paste(metadata_table$sponge_id, metadata_table$method, sep = "_")
categories <- unique(metadata_table$distinct)
matrix_list <- list(data = list())
for (category in categories) {
  matrix_list[["data"]][[category]] <- as.matrix(count_table[, rownames(metadata_table[metadata_table$distinct == category, ]), drop = FALSE])
}

## run iNEXT3D
inext_out <- iNEXT3D(data = matrix_list$data, diversity = 'TD', q = c(0, 1, 2), datatype = 'incidence_raw', nboot = 50)

## explore results
inext_out$TDInfo
inext_out$TDAsyEst
ggiNEXT3D(inext_out, type = 1, facet.var = 'Assemblage') + 
  facet_wrap(~Assemblage, nrow = 3) +
  theme(title = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.spacing = unit(1, "lines")) +
  scale_x_continuous(limits = c(0, 20), 
                     breaks = seq(0, 20, by = 5),
                     expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 40),
                     breaks = seq(0, 40, by = 10),
                     expand = c(0, 0))
estimate3D(matrix_list$data, diversity = 'TD', q = 0, datatype = 'incidence_raw', base = 'coverage', level = 0.9)
```

### 6.4 Basic sample read and ZOTU stats

Before analysing alpha and beta diversity, we need to generate some numbers representing read counts and ZOTU detections across samples for the first paragraph of the results section in the manuscript.

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(dplyr)
library(Rmisc)
library(ggplot2)
library(car)

## read data into R
count_table <- read.table('clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_table.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')
raw_metadata_table <- read.table('ethanol_comparison_heDNA_metadata.txt', header = TRUE, sep = '\t', row.names = 1, check.names = FALSE, comment.char = '')

## total number of reads and ZOTUs
sum(count_table)
nrow(count_table)

## average number of reads per sample
raw_metadata_table$total_read_count <- colSums(count_table)[rownames(raw_metadata_table)]
raw_metadata_table <- raw_metadata_table %>%
  filter(sponge_id != 'negative')
mean(raw_metadata_table$total_read_count, na.rm = TRUE)
sd(raw_metadata_table$total_read_count, na.rm = TRUE) / sqrt(sum(!is.na(raw_metadata_table$total_read_count)))

## two-way ANOVA
raw_metadata_table$total_read_count_filled <- raw_metadata_table$total_read_count
raw_metadata_table$total_read_count_filled[is.na(raw_metadata_table$total_read_count_filled)] <- 0
sum <- summarySE(raw_metadata_table, measurevar = "total_read_count_filled",
          groupvars = c("sponge_id", "method"))
pd <- position_dodge(.2)
ggplot(sum, aes(x = method, y = total_read_count_filled, color = sponge_id)) +
  geom_errorbar(aes(ymin = total_read_count_filled - se, ymax = total_read_count_filled + se), width = 0.2, size = 0.7, position = pd) +
  geom_point(shape = 15, size = 4, position = pd) +
  theme_bw() +
  theme(axis.title.y = element_text(vjust = 1.8),
        axis.title.x = element_text(vjust = -0.5),
        axis.title = element_text(face = "bold"))
boxplot(total_read_count_filled ~ method,
        data = raw_metadata_table,
        xlab = "treatment",
        ylab = "read count")
boxplot(total_read_count_filled ~ sponge_id,
        data = raw_metadata_table,
        xlab = "treatment",
        ylab = "read count")
boxplot(total_read_count_filled ~ method:sponge_id,
        data = raw_metadata_table,
        xlab = "treatment",
        ylab = "read count")
model = lm(total_read_count_filled ~ method + sponge_id + method:sponge_id,
           data = raw_metadata_table)
Anova(model, type = 'II')
anova(model)
summary(model)

## check assumptions
hist(residuals(model),
     col = "darkgray")
plot(fitted(model),
     residuals(model))
```

## 7. Phylogenetic tree

```{code-block} R
## prepare R environment
setwd("/Users/gjeunen/Documents/work/research_projects/2022_marsden/objective_1/ethanolComparison/10.final")
library(Biostrings)
library(DECIPHER)
library(ape)

## read data into R using Biostrings
sequence_table <- readDNAStringSet('clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_asvs.fasta')

## generate alignment using DECIPHER
alignment <- AlignSeqs(sequence_table, anchor = NA)

## export alignment in nexus format using ape
write.nexus.data(alignment, 'clean_contaminant_removed_tombRaider_ethanol_comparison_heDNA_asvs.nex')

# The next steps are not conducted in R.
# a) generate the .xml file using BEAUTi 2. 
#       a.1) Import nexus alignment
#       a.2) Set substitution model to HKY
#       a.3) Set starting tree to Cluster Tree and cluster type to UPGMA
#       a.4) Set Chain length to 10^8 and log every 10,000 trees
#       a.5) Keep remaining settings on default and export .xml file
# b) run BEAST 2 to generate phylogenetic trees.
# c) after BEAST 2 completes, check log file in Tracer.
# d) find best supported tree using TreeAnnotator and export to .tree file.
# e) visualise tree quality using FigTree.
```

## 8. Statistical analysis

### 8.1 DNA concentration and purity

Total DNA concentration and purity were measured for each eDNA extract using the Denovix DS-11. Measurements were recorded and stored in file `qubitAndDenovixMeasurementsAllSponges.xlsx`. The following R script reads in the file, parses the data to remove outliers, conducts the statistical analysis reported in the manuscript, and generates Figure 2 of the manuscript.

```{code-block} R
## prepare R environment
rm(list = ls())
setwd("/Volumes/LaCie/2022_Marsden/objective_1/ethanolComparison")
required.libraries <- c("readxl", "ggplot2", "tidyr", "outliers", "ggpubr", "car", "rstatix", "aplot")
lapply(required.libraries, require, character.only = TRUE)

## read data into R
data <- read_xlsx('qubitAndDenovixMeasurementsAllSponges.xlsx', sheet = 'Sheet1', na = '')

## wrangle data in correct format for statistical analysis and plotting
colSums(is.na(data))
data <- data[!is.na(data$Group), ]
colSums(is.na(data))
data$spongeID <- substr(data$Group, nchar(data$Group) -1, nchar(data$Group))
data$treatment <- sub("^([^_]+)_.*$", "\\1", data$`Sample ID`)
data <- data[which(data$treatment != 'FP'), ]
data <- data[which(data$spongeID != '92'), ]
data <- data[which(data$Group != 'NEG'), ]
str(data, width = 50, strict.width = 'cut')
data$`Original Sample Conc.` <- as.numeric(data$`Original Sample Conc.`)
str(data, width = 50, strict.width = 'cut')

## rename categorical variables for plotting
data$spongeID <- dplyr::recode(data$spongeID, '24' = 'cat # 37124', '35' = 'cat # 37535', '74' = 'cat # 35574')
data$treatment <- dplyr::recode(data$treatment, 'F0' = 'F10')

## identify and remove outliers across the whole data set
data %>%
  ggplot(aes(y = `Original Sample Conc.`, x = Group)) + labs(y = 'DNA conc.', x = 'sponge + treatment') +
  geom_boxplot(fill = 'grey88')
data %>%
  ggplot(aes(y = `260/280`, x = Group)) + labs(y = '260/280', x = 'sponge + treatment') +
  geom_boxplot(fill = 'grey88')
grubbs.test(data$`260/280`, type = 11)
data[data$`260/280` < -10 | data$`260/280` > 10, c("Sample ID", "260/280")]
data$`260/280`[data$`260/280` < -10 | data$`260/280` > 10] <- NA
data %>%
  na.omit() %>%
  ggplot(aes(y = `260/280`, x = Group)) + labs(y = '260/280', x = 'sponge + treatment') +
  geom_boxplot(fill = 'grey88')
data %>%
  ggplot(aes(y = `260/230`, x = Group)) + labs(y = '260/230', x = 'sponge + treatment') +
  geom_boxplot(fill = 'grey88')
grubbs.test(data$`260/230`, type = 11)
data[data$`260/230` < -5 | data$`260/230` > 5, c("Sample ID", "260/230")]
data$`260/230`[data$`260/230` < -4 | data$`260/230` > 4] <- NA
data %>%
  na.omit() %>%
  ggplot(aes(y = `260/230`, x = Group)) + labs(y = '260/230', x = 'sponge + treatment') +
  geom_boxplot(fill = 'grey88')

## check for ANOVA assumptions 
## (easiest to do this with for loops, as we need to test for three sponges and three measurements)
## normality of residuals, homogeneity of the response variable, and unbalanced design
spongeIDs <- c("cat # 37124", "cat # 37535", "cat # 35574")
factors <- c("Original Sample Conc.", "260/280", "260/230")
for (sponge in spongeIDs) {
  for (factor in factors) {
    subset_data <- data[data$spongeID == sponge, ]
    aov_model <- aov(as.formula(paste0("`", factor, "` ~ as.factor(Group)")), data = subset_data)
    residuals_data <- residuals(aov_model)
    print(
      ggdensity(residuals_data,
                main = paste("Density plot of residuals", sponge, "-", factor),
                xlab = "Residuals")
    )
    print(
      ggqqplot(residuals_data,
               main = paste("Q-Q plot of residuals", sponge, "-", factor))
    )
    hist(residuals_data,
         main = paste("Histogram of residuals", sponge, "-", factor),
         xlab = "Residuals", breaks = 20, col = "lightblue", border = "black")
    shapiro_test <- shapiro.test(residuals_data)
    print(paste("Shapiro-Wilk test for", sponge, "-", factor))
    print(shapiro_test)
    levenes_test <- leveneTest(as.formula(paste0("`", factor, "` ~ as.factor(Group)")), data = subset_data)
    bartlett_test <- bartlett.test(as.formula(paste0("`", factor, "` ~ as.factor(Group)")), data = subset_data)
    print(paste("Levene test for", sponge, "-", factor))
    print(levenes_test)
    print(paste("Bartlett test for", sponge, "-", factor))
    print(bartlett_test)
    table(subset_data$Group[!is.na(subset_data[[factor]])])
  }
}

## check for an excess of zeros in the response variable
sum(data$`Original Sample Conc.` == 0, na.rm = TRUE) * 100 / nrow(data)
sum(data$`260/280` == 0, na.rm = TRUE) * 100 / nrow(data)
sum(data$`260/230` == 0, na.rm = TRUE) * 100 / nrow(data)

## run non-parametric Welch's ANOVA given significant Shapiro-Wilk and levene's tests.
## since we are mainly interested in differences between treatments and not between sponges,
## we will run the ANOVA once per sponge specimen to simplify the analysis and interpretation of the results.
# the non-parametric version of a one-way ANOVA for heteroscedasticity is the Welch's ANOVA,
# followed by the non-parametric post hoc Games-Howell test.
# I need to keep in mind to adjust the p-values using the Benjamini-Hochberg method afterwards.
Anova(lm(`Original Sample Conc.` ~ treatment, data = na.omit(data[which(data$spongeID == 'cat # 37124'), ])), Type='II', white.adjust = TRUE)
gh.24.dna <- games_howell_test(data = na.omit(data[which(data$spongeID == 'cat # 37124'), ]), formula = `Original Sample Conc.` ~ treatment, detailed = TRUE)
Anova(lm(`260/280` ~ treatment, data = na.omit(data[which(data$spongeID == 'cat # 37124'), ])), Type='II', white.adjust = TRUE)
gh.24.280 <- games_howell_test(data = na.omit(data[which(data$spongeID == 'cat # 37124'), ]), formula = `260/280` ~ treatment, detailed = TRUE)
Anova(lm(`260/230` ~ treatment, data = na.omit(data[which(data$spongeID == 'cat # 37124'), ])), Type='II', white.adjust = TRUE)
gh.24.230 <- games_howell_test(data = na.omit(data[which(data$spongeID == 'cat # 37124'), ]), formula = `260/230` ~ treatment, detailed = TRUE)

Anova(lm(`Original Sample Conc.` ~ treatment, data = na.omit(data[which(data$spongeID == 'cat # 37535'), ])), Type='II', white.adjust = TRUE)
gh.35.dna <- games_howell_test(data = na.omit(data[which(data$spongeID == 'cat # 37535'), ]), formula = `Original Sample Conc.` ~ treatment, detailed = TRUE)
Anova(lm(`260/280` ~ treatment, data = na.omit(data[which(data$spongeID == 'cat # 37535'), ])), Type='II', white.adjust = TRUE)
gh.35.280 <- games_howell_test(data = na.omit(data[which(data$spongeID == 'cat # 37535'), ]), formula = `260/280` ~ treatment, detailed = TRUE)
Anova(lm(`260/230` ~ treatment, data = na.omit(data[which(data$spongeID == 'cat # 37535'), ])), Type='II', white.adjust = TRUE)
gh.35.230 <- games_howell_test(data = na.omit(data[which(data$spongeID == 'cat # 37535'), ]), formula = `260/230` ~ treatment, detailed = TRUE)

Anova(lm(`Original Sample Conc.` ~ treatment, data = na.omit(data[which(data$spongeID == 'cat # 35574'), ])), Type='II', white.adjust = TRUE)
gh.74.dna <- games_howell_test(data = na.omit(data[which(data$spongeID == 'cat # 35574'), ]), formula = `Original Sample Conc.` ~ treatment, detailed = TRUE)
Anova(lm(`260/280` ~ treatment, data = na.omit(data[which(data$spongeID == 'cat # 35574'), ])), Type='II', white.adjust = TRUE)
gh.74.280 <- games_howell_test(data = na.omit(data[which(data$spongeID == 'cat # 35574'), ]), formula = `260/280` ~ treatment, detailed = TRUE)
Anova(lm(`260/230` ~ treatment, data = na.omit(data[which(data$spongeID == 'cat # 35574'), ])), Type='II', white.adjust = TRUE)
gh.74.230 <- games_howell_test(data = na.omit(data[which(data$spongeID == 'cat # 35574'), ]), formula = `260/230` ~ treatment, detailed = TRUE)

# Seventh, create three sets of box plots to visualise the differences between treatments for:
# a) total DNA concentration
# b) 260/280 ratios
# c) 260/230 ratios
# At first, we will set the colours and shape.
treatment_colors <- c("C" = "#E0908D", "E" = "#f2d379", "F10" = "#4e6c82", "F1" = "#c4d8e1", "P" = "#806491", "T" = "grey", "TW" = "black")
sample_shape <- c('cat # 35574' = 21, 'cat # 37124' = 22, 'cat # 37535' = 24)

# Then, we will create the boxplot for total DNA concentration,
# followed by the 260/280 ratio and the 160/230 ratio.
dna_conc <- ggplot(data, aes(x = treatment, y = `Original Sample Conc.`, shape = spongeID)) +
  geom_boxplot(outlier.shape = NA, width = 0.8) +
  geom_jitter(aes(fill = treatment, shape = spongeID), size = 3, width = 0.1, alpha = 0.6) +
  scale_fill_manual(values = treatment_colors) +
  facet_wrap(~spongeID, ncol = length(unique(data$spongeID))) +
  scale_y_log10(limits = c(0.0001,100)) +
  scale_shape_manual(values = sample_shape) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title.x = element_blank(),
        axis.ticks.x = element_blank(), axis.text.x = element_blank())
dna_conc

ratio_280 <- ggplot(na.omit(data), aes(x = treatment, y = `260/280`, shape = spongeID)) +
  geom_boxplot(outlier.shape = NA, width = 0.8) +
  geom_jitter(aes(fill = treatment, shape = spongeID), size = 3, width = 0.1, alpha = 0.6) +
  scale_fill_manual(values = treatment_colors) +
  facet_wrap(~spongeID, ncol = length(unique(data$spongeID))) +
  scale_y_continuous(limits = c(-10,10)) +
  scale_shape_manual(values = sample_shape) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title.x = element_blank(),
        axis.ticks.x = element_blank(), axis.text.x = element_blank(), strip.text = element_blank())
ratio_280

ratio_230 <- ggplot(na.omit(data), aes(x = treatment, y = `260/230`, shape = spongeID)) +
  geom_boxplot(outlier.shape = NA, width = 0.8) +
  geom_jitter(aes(fill = treatment, shape = spongeID), size = 3, width = 0.1, alpha = 0.6) +
  scale_fill_manual(values = treatment_colors) +
  facet_wrap(~spongeID, ncol = length(unique(data$spongeID))) +
  scale_y_continuous(limits = c(-5,5)) +
  scale_shape_manual(values = sample_shape) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.text = element_blank())
ratio_230

# combine the three graphs together using aplot.
ratio_280 %>% insert_top(dna_conc) %>% insert_bottom(ratio_230)
```

### 8.2 Alpha diversity

### 8.3 Beta diversity

## 9. Map of Antarctica

The first figure of the manuscript is a map of Antarctica displaying the locations of the three specimens which were analysed in this experiment. To generate the map, we can run the R script below. To successfully execute the script, several files will need to be downloaded, including:

1. A high resolution Antarctic outline including ice sheets:
   1. source: <https://data.bas.ac.uk/full-record.php?id=GB/NERC/BAS/PDC/01391>
   2. filename: add_coastline_high_res_polygon_v7.2.shp
2. A csv file containing the specimen metadata
   1. filename: specimen_metadata.csv
3. A high resolution bathymetry data file as baselayer:
   1. source: <https://www.gebco.net>
   2. filename: IBCSO_v2_ice-surface.nc
4. A world map with borders for all countries:
   1. source: <https://www.star.nesdis.noaa.gov/data/smcd1/vhp/GIS/TM_World_Borders/>
   2. filename: TM_WORLD_BORDERS-0.3.shp

All files needed to execute the R code can be found in the subdirectory `metadata/figures/figure_1/` within this GitHub repo. Once the figure is generated in R and exported to a pdf file, import figure into Adobe Illustrator to clean up figure for publication.

```{code-block} R
#######################
## MAP OF ANTARCTICA ##
#######################
# set working directory and load libraries
setwd("")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(terra, tidyterra, ggplot2, ggnewscale, patchwork)
require(sf)
library(recolorize)
library(egg)
library(png)
library(ggpubr)
library(viridis)
library(RColorBrewer) 
library(dplyr)
library(hrbrthemes)
library(readxl)
library(ggrepel)
library(pracma)
library(scales)

# read data
ATAshp <- simplifyGeom(vect("add_coastline_high_res_polygon_v7.2/add_coastline_high_res_polygon_v7.2.shp"), tolerance=500)
ATAshp$col <- sapply(ATAshp$surface, function(x){ifelse(x == "land", "grey85", "grey98")})
sponge_dat <- read.csv("specimen_metadata2.csv")
sponge_pts <- project(vect(sponge_dat, geom = c("Longitude1", "Latitude1"), keepgeom = TRUE, crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"), ATAshp)
grat <- project(vect(sf::st_graticule(lon=seq(-175, 180, 5), lat = seq(-85, -60, 5), ndiscr = 5000)), ATAshp)
ibsco <- rast("add_coastline_high_res_polygon_v7.2/IBCSO_v2_ice-surface.nc")
sample_colors <- c("Calcarea" = "lightgoldenrod", "Demospongiae" = "steelblue", "Hexactinellida" = "firebrick")
spp_names <- c(expression(italic("Calcarea")), expression(italic("Demospongiae")), expression(italic("Hexactinellida")))
sponge_pts$Catalog.Number <- as.factor(sponge_pts$Catalog.Number)
xmn <- -1000000
xmx <- 1500000
ymn <- -2500000
ymx <- -1000000
WORLDshp <- simplifyGeom(vect("TM_WORLD_BORDERS-0.3/TM_WORLD_BORDERS-0.3.shp"))
WORLDshpNoATA <- WORLDshp[WORLDshp$ISO3 != "ATA"]

# generate circular masking layer for overview plot
prj <- "+proj=stere +lat_0=-90 +lat_ts=-71 +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=km +no_defs +ellps=WGS84 +towgs84=0,0,0"
little <- st_point(c(0,0)) %>% st_sfc(crs = prj) %>% st_buffer(dist = 4530)
xlim <- sf::st_bbox(little)[c("xmin", "xmax")]*2.5
ylim <- sf::st_bbox(little)[c("ymin", "ymax")]*2.5
encl_rect <- list(cbind(c(xlim[1], xlim[2], xlim[2], xlim[1], xlim[1]), 
                        c(ylim[1], ylim[1], ylim[2], ylim[2], ylim[1]))) %>%
  sf::st_polygon() %>%
  sf::st_sfc(crs = prj)
circleMask <- sf::st_difference(encl_rect, little)

# generate plots
RSR_plot <- ggplot() + 
  geom_spatraster(data = ibsco, show.legend = FALSE) + 
  scale_fill_gradient(low = "#528B8B", high = "#CBDCDC") + 
  geom_spatvector(data = grat, col = "grey50", alpha = 0.2) + 
  geom_spatvector(data = ATAshp, alpha = 1, fill = ATAshp$col, col = 'grey20') + 
  new_scale_fill() + 
  geom_spatvector(data = sponge_pts, aes(shape = Class, fill = Class), alpha = 1, size = 5, stroke = 0.5) + 
  scale_fill_manual(values = sample_colors, labels = c("Calcarea", "Demospongiae", "Hexactinellida") , name = "Sponge Class") +
  guides(fill = guide_legend(override.aes = list(pch=21))) + 
  scale_shape_manual(values = c(21, 22, 23)) + 
  scale_x_continuous(breaks = seq(-180, 180, by = 10)) + 
  coord_sf(crs = crs(ATAshp), expand = FALSE, xlim = c(xmn, xmx), ylim = c(ymn, ymx)) + 
  annotate(geom = "rect", xmin = xmn, xmax = xmx, ymin = ymn, ymax = ymx, fill = NA, col = "grey10") +  
  xlab(NULL) + ylab(NULL) #  

inset_map <- ggplot() + 
  geom_spatvector(data = ATAshp, alpha = 1, fill = ATAshp$col, col = 'grey20') + 
  geom_rect(aes(xmin = xmn, xmax = xmx, ymin = ymn, ymax = ymx), fill = "dark red", col = "black", alpha = 0.2) + 
  theme(panel.grid.minor = element_blank(), panel.grid.major = element_blank(), panel.background = element_blank(), plot.background = element_blank(), panel.border = element_blank(), axis.line = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), plot.title = element_blank()) 

#output to pdf
pdf("RSR_spongemap_v2.pdf", width = 8, height = 5.5) 
RSR_plot + annotation_custom(ggplotGrob(inset_map), xmin = xmx - (xmx-xmn)/3, xmax = xmx, ymin = ymx - (ymx-ymn)/3, ymax = ymx + 280000) 
dev.off() 
```
